1. sound frequency cannot be determined from 1 sample
  a. you must have multiple samples put in at once to get a frequency. This means that the data that does into the LSTM cannot have shape (1,), but it must be something like (1,10) so it has some room to work with
2. the raw audio data must be splitted up into multiple smaller sections to avoid overfitting and long training durations.
3. the data is limited, so we must augment it by either manipulating its speed, pitch, or amplitudes
4. amplitudes in the raw audio file does not affect the resulting frequency as much as the resulting output is independent that of the amplitude
5. there's gonna be alot of bitch works
6. data processing transcribe to CPP code

TODO:
1. aquire sample data and see if model works
2. find out how to write out midi files
3. find full data sections

PLAN:


File type into LSTM:
Format:
1| TEMPO={tempo}
2| TRACK_START
3| {time_start}.{time_end}.{[freq1, freq2]/{0}}
4| ...
5| {time_start}.{time_end}.{[freq1, freq2]/{0}}
6| TRACK_END

output encoding: ONE_HOT (128 slots)

(C_1)
Time event calculation:
Absolute time is based on 1/16th of a MIDI clock. Calculation are as follow:
1_clock_period = ((tempo/1,000,000)/24)/16
absolute_time{seconds} = clock_period * clock_repetition

LSTM dev:
UNIVERSAL time constant: seconds
ex:
WAV sample rate = 44.1khz = 0.000022676 s/sample, denoted as T_W
MIDI sample tempo = 500,000 = 0.00130283 s/sample, denoted as T_M (check C_1 for calculation)
for every passed in WAV file, a midi note of same time constant (T_W) is produced
however, when calculating the error between the outputted data and the given data, the timeframe will not match up, as the given output has time constant T_M
To match up time constant, there are 2 methods:
1. Create custom error function that force match the time constants
2. estimate the range of the LSTM output, and averaging them so the outputs time constant can match up, then use mean square error or some deviation errors.
  a. instead of calculating the average, we can also take the largest value

Pros and Cons (only an estimate):
1. custom error functions are highly versatile and can be manipulated easily, but can also easily count in outliers as part of the error
2. Averaging or taking the max will have time offset issues, which can induce unwanted error shifts, and also ignores outliers. But can process faster and let the LSTM focus a bit more on the abstract side of generation.

Check calculation.png for more details

Loss graph format:
Train binary_crossentropy
Train mean_squared_error
Val binary_crossentropy
Val mean_squared_error

Optimal network structure and training parameters as of now:
1.
Sequential
  |_ Model structure:
  |   |_ LSTM(128, batch_input_shape=((None, 1, compression_size)), activation='relu', return_sequences=True)
  |   |_ Dense(128, activation='tanh')
  |   |_ Using CuDNN?
  |   |   |_ No
  |   |_ Avg training cycle time: 10s
  |
  |_ Opt = Adam(lr=1e-2, decay=1e-5)
  |   |_ Loss = mean_squared_error
  |
  |_ Lowest loss: 0.02
  |_ RESULT: Overfitted horribly

Research Journal 3-29:
Experiments are done on the effectiveness of loss functions on large onehot-encoded datasets
On average, the binary_crossentropy loss function performed poorly on most occasions, but can be used as an accurate representation of the deviation of the output to the ground truth. The softmax_cross_entropy_with_logits, as well as categorical_crossentropy performed similarly, with the categorical_crossentropy having a noticable 50% decrease in loss in smaller output to ground truth deviations. A new loss function was then created in the attempt to combine the benefits of both loss functions. The new function is depicted as follow:

Assume that the output vector generated by CATCSE is x, and BINCSE is y, and a constant C which will act as a scaler vector
new_loss = ((x*c)*y)
Optimal scaler has not been tested yet, although optimal scaler seems to be close to the constant E.

This function attempts to scale down the CATCSE values as the deviation gets smaller, which in a way, the BINCSE is controlling the overall loss at smaller deviations. However, at larger deviations, under normal circumstances, BINCE approaches close to 0.6-0.7. And after the application of the constant scaler, acts as a new scaler vector of 1.2-1.4 for the CATCSE loss, which can more accuratly represent the overall deviation fron the ground truth.

Journal end
I am fucking tired
